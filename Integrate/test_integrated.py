import cv2
import mediapipe as mp
import pyautogui
import numpy as np
import time

# --- åˆæœŸåŒ– ---

# ç”»é¢ã‚µã‚¤ã‚ºã®å–å¾—
screen_w, screen_h = pyautogui.size()

# MediaPipeã®ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5)

mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    max_num_hands=1,
    min_detection_confidence=0.7,
    min_tracking_confidence=0.5)

mp_draw = mp.solutions.drawing_utils

# Webã‚«ãƒ¡ãƒ©ã®åˆæœŸåŒ–
cap = cv2.VideoCapture(0)

# --- è¦–ç·šè¿½è·¡ç”¨ã®å¤‰æ•° ---
calibration_points = []
gaze_min = [1.0, 1.0]
gaze_max = [0.0, 0.0]
alpha = 0.3
calibrated = False

# ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ã‚¬ã‚¤ãƒ‰ãƒ†ã‚­ã‚¹ãƒˆ
guide_texts = [
    "Look at the TOP LEFT corner and press SPACE",
    "Look at the TOP RIGHT corner and press SPACE",
    "Look at the BOTTOM LEFT corner and press SPACE",
    "Look at the BOTTOM RIGHT corner and press SPACE",
    "Look at the CENTER and press SPACE"
]

# --- ãƒãƒ³ãƒ‰ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ç”¨ã®å¤‰æ•° ---
gesture_state = "Ready"
current_action = "None"
scroll_cooldown = 0
dragging = False
tips_ids = [4, 8, 12, 16, 20] # å„æŒ‡å…ˆã®ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ID

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°: æŒ‡ã®é–‹é–‰ã‚’æ¤œå‡º ---
def detect_fingers_up(landmarks):
    """
    æ‰‹ã®ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æƒ…å ±ã‹ã‚‰ã€å„æŒ‡ãŒä¸ŠãŒã£ã¦ã„ã‚‹ã‹ï¼ˆé–‹ã„ã¦ã„ã‚‹ã‹ï¼‰ã‚’åˆ¤å®šã™ã‚‹é–¢æ•°
    """
    fingers = []
    # è¦ªæŒ‡: xåº§æ¨™ã§åˆ¤å®šï¼ˆæ‰‹ã®å‘ãã«ä¾å­˜ã™ã‚‹ãŸã‚å˜ç´”åŒ–ï¼‰
    # è¦ªæŒ‡ã®å…ˆç«¯(4)ãŒã€ãã®ä»˜ã‘æ ¹ã«è¿‘ã„ç‚¹(3)ã‚ˆã‚Šå¤–å´ã«ã‚ã‚Œã°é–‹ã„ã¦ã„ã‚‹ã¨åˆ¤å®š
    if landmarks.landmark[tips_ids[0]].x < landmarks.landmark[tips_ids[0] - 1].x:
        fingers.append(True)
    else:
        fingers.append(False)

    # äººå·®ã—æŒ‡ã‹ã‚‰å°æŒ‡: yåº§æ¨™ã§åˆ¤å®š
    # å„æŒ‡ã®å…ˆç«¯ãŒã€2ã¤ä¸‹ã®é–¢ç¯€ã‚ˆã‚Šã‚‚ä¸Šã«ã‚ã‚Œã°é–‹ã„ã¦ã„ã‚‹ã¨åˆ¤å®š
    for i in range(1, 5):
        if landmarks.landmark[tips_ids[i]].y < landmarks.landmark[tips_ids[i] - 2].y:
            fingers.append(True)
        else:
            fingers.append(False)
    return fingers

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---

print("ğŸ”§ ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
print("Webã‚«ãƒ¡ãƒ©ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã«ã—ã¦ã€ç”»é¢ã®æŒ‡ç¤ºã«å¾“ã£ã¦ãã ã•ã„ã€‚")

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # æ˜ åƒã‚’å·¦å³åè»¢ã—ã€å‡¦ç†ã®ãŸã‚ã«BGRã‹ã‚‰RGBã«å¤‰æ›
    frame = cv2.flip(frame, 1)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    img_h, img_w, _ = frame.shape

    # é¡”ã¨æ‰‹ã®ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã‚’æ¤œå‡º
    face_results = face_mesh.process(rgb_frame)
    hand_results = hands.process(rgb_frame)

    # --- 1. è¦–ç·šè¿½è·¡ã¨ã‚«ãƒ¼ã‚½ãƒ«ç§»å‹• ---
    if face_results.multi_face_landmarks:
        face_landmarks = face_results.multi_face_landmarks[0]
        
        # å³ç›®ã®ç³å­”ã®ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯(473)ã‚’ä½¿ç”¨
        gaze_x = face_landmarks.landmark[473].x
        gaze_y = face_landmarks.landmark[473].y

        if not calibrated:
            # --- ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚§ãƒ¼ã‚º ---
            idx = len(calibration_points)
            if idx < 5:
                # ç”»é¢ã«ã‚¬ã‚¤ãƒ‰ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º
                guide_text = guide_texts[idx]
                cv2.putText(frame, guide_text, (30, 60),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
            
            key = cv2.waitKey(1) & 0xFF
            if key == 32:  # ã‚¹ãƒšãƒ¼ã‚¹ã‚­ãƒ¼ãŒæŠ¼ã•ã‚ŒãŸã‚‰
                calibration_points.append((gaze_x, gaze_y))
                print(f"ğŸ“Œ ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚¤ãƒ³ãƒˆ {idx+1}/5 ã‚’è¨˜éŒ²ã—ã¾ã—ãŸã€‚")
                
                if len(calibration_points) == 5:
                    # 5ç‚¹é›†ã¾ã£ãŸã‚‰ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†
                    gaze_min[0] = min(p[0] for p in calibration_points)
                    gaze_max[0] = max(p[0] for p in calibration_points)
                    gaze_min[1] = min(p[1] for p in calibration_points)
                    gaze_max[1] = max(p[1] for p in calibration_points)
                    calibrated = True
                    print("âœ… ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ï¼ ãƒã‚¦ã‚¹æ“ä½œã‚’é–‹å§‹ã—ã¾ã™ã€‚")
                    # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¬ã‚¤ãƒ‰ã‚’æ¶ˆã™ãŸã‚ã«å°‘ã—å¾…ã¤
                    cv2.waitKey(500) 
        
        else:
            # --- ãƒã‚¦ã‚¹ç§»å‹•ãƒ•ã‚§ãƒ¼ã‚º ---
            # è¦–ç·šåº§æ¨™ã‚’ç”»é¢åº§æ¨™ã«ãƒãƒƒãƒ”ãƒ³ã‚°
            # ç¯„å›²å¤–ã®å€¤ãŒå‡ºãªã„ã‚ˆã†ã«ã‚¯ãƒªãƒƒãƒ—
            norm_x = np.clip((gaze_x - gaze_min[0]) / (gaze_max[0] - gaze_min[0]), 0, 1)
            norm_y = np.clip((gaze_y - gaze_min[1]) / (gaze_max[1] - gaze_min[1]), 0, 1)
            
            screen_x = screen_w * norm_x
            screen_y = screen_h * norm_y

            # ã‚«ãƒ¼ã‚½ãƒ«ã®å‹•ãã‚’æ»‘ã‚‰ã‹ã«ã™ã‚‹ (ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°)
            prev_x, prev_y = pyautogui.position()
            # alpha = 0.3 # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ä¿‚æ•°ï¼ˆå°ã•ã„ã»ã©æ»‘ã‚‰ã‹ï¼‰
            smooth_x = prev_x * (1 - alpha) + screen_x * alpha
            smooth_y = prev_y * (1 - alpha) + screen_y * alpha

            # ãƒ‰ãƒ©ãƒƒã‚°ä¸­ã§ãªã‘ã‚Œã°ã‚«ãƒ¼ã‚½ãƒ«ã‚’ç§»å‹•
            pyautogui.moveTo(smooth_x, smooth_y)
            
            # ãƒ‡ãƒãƒƒã‚°ç”¨ã«è¦–ç·šä½ç½®ã‚’å††ã§è¡¨ç¤º
            cv2.circle(frame, (int(gaze_x * img_w), int(gaze_y * img_h)), 5, (0, 255, 0), -1)

    # --- 2. ãƒãƒ³ãƒ‰ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã«ã‚ˆã‚‹æ“ä½œ (ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†å¾Œã®ã¿) ---
    current_action = "None"
    if calibrated and hand_results.multi_hand_landmarks:
        for handLms in hand_results.multi_hand_landmarks:
            # ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼èªè­˜ã®å‰ã«ç¾åœ¨ã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
            gesture_state = "Ready"

            fingers = detect_fingers_up(handLms)

            # --- ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼åˆ¤å®š ---
            if fingers == [True, False, False, False, False]:  # è¦ªæŒ‡ã®ã¿ -> ä¸Šã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                current_action = "Scroll Up"
                if scroll_cooldown == 0:
                    pyautogui.scroll(10)
                    scroll_cooldown = 10 # ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³è¨­å®š

            elif fingers == [False, False, False, False, True]: # å°æŒ‡ã®ã¿ -> ä¸‹ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                current_action = "Scroll Down"
                if scroll_cooldown == 0:
                    pyautogui.scroll(-10)
                    scroll_cooldown = 10 # ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³è¨­å®š

            elif fingers == [False, True, False, False, False]: # äººå·®ã—æŒ‡ã®ã¿ -> ã‚¯ãƒªãƒƒã‚¯
                current_action = "Click"
                if gesture_state != "click":
                    pyautogui.click()
                    gesture_state = "click"

            elif fingers == [False, True, True, False, False]:  # äººå·®ã—æŒ‡ã¨ä¸­æŒ‡ -> ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯
                current_action = "Double Click"
                if gesture_state != "double":
                    pyautogui.doubleClick()
                    gesture_state = "double"
            
            elif fingers == [True, True, False, False, False]: # è¦ªæŒ‡ã¨äººå·®ã—æŒ‡ -> å³ã‚¯ãƒªãƒƒã‚¯
                current_action = "Right Click"
                if gesture_state != "right":
                    pyautogui.rightClick()
                    gesture_state = "right"

            elif fingers == [False, False, False, False, False]:  # ã‚°ãƒ¼ -> ãƒ‰ãƒ©ãƒƒã‚°é–‹å§‹
                current_action = "Dragging"
                if not dragging:
                    pyautogui.mouseDown()
                    dragging = True
                gesture_state = "drag"

            elif fingers == [True, True, True, True, True]:  # ãƒ‘ãƒ¼ -> åŸºæœ¬çŠ¶æ…‹ / ãƒ‰ãƒ©ãƒƒã‚°çµ‚äº†
                current_action = "Ready"
                if dragging:
                    pyautogui.mouseUp()
                    dragging = False
                gesture_state = "Ready"

            # æ‰‹ã®ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã‚’æç”»
            mp_draw.draw_landmarks(frame, handLms, mp_hands.HAND_CONNECTIONS)
    
    if current_action == "None": alpha = 0.3
    else: alpha = 0.1
    
    # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³å‡¦ç†
    if scroll_cooldown > 0:
        scroll_cooldown -= 1

    # --- ç”»é¢è¡¨ç¤º ---
    if calibrated:
        cv2.putText(frame, f"Action: {current_action}", (10, 40),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 200, 255), 2)

    cv2.imshow("Gaze and Gesture Mouse Control", frame)
    
    # 'q'ã‚­ãƒ¼ã¾ãŸã¯ESCã‚­ãƒ¼ã§çµ‚äº†
    key = cv2.waitKey(1) & 0xFF
    if key == ord('q') or key == 27:
        break

# --- çµ‚äº†å‡¦ç† ---
cap.release()
cv2.destroyAllWindows()